diff --git a/arch/x86/syscalls/syscall_64.tbl b/arch/x86/syscalls/syscall_64.tbl
index c6b1de2..9299f62 100644
--- a/arch/x86/syscalls/syscall_64.tbl
+++ b/arch/x86/syscalls/syscall_64.tbl
@@ -326,6 +326,8 @@
 317	common	seccomp			sys_seccomp
 318     64      setcolors               sys_setcolors
 319     64      getcolors               sys_getcolors
+320     64      ssmem_attach            sys_ssmem_attach
+321     64      ssmem_detach            sys_ssmem_detach
 
 #
 # x32-specific system call numbers start at 512 to avoid cache impact
diff --git a/include/linux/mm_types.h b/include/linux/mm_types.h
index 875ba48..508f0d9 100644
--- a/include/linux/mm_types.h
+++ b/include/linux/mm_types.h
@@ -283,6 +283,9 @@ struct vm_area_struct {
 	const struct vm_operations_struct *vm_ops;
 
 	/* Information about our backing store: */
+
+	int ssmem_id;
+
 	unsigned long vm_pgoff;		/* Offset (within vm_file) in PAGE_SIZE
 					   units, *not* PAGE_CACHE_SIZE */
 	struct file * vm_file;		/* File we map to (can be NULL). */
diff --git a/include/linux/syscalls.h b/include/linux/syscalls.h
index 00de918..aa78b09 100644
--- a/include/linux/syscalls.h
+++ b/include/linux/syscalls.h
@@ -845,4 +845,6 @@ asmlinkage long sys_seccomp(unsigned int op, unsigned int flags,
 			    const char __user *uargs);
 asmlinkage long sys_setcolors(int nr_pids, pid_t *pids, u_int16_t *colors, int *retval);
 asmlinkage long sys_getcolors(int nr_pids, pid_t *pids, u_int16_t *colors, int *retval);
+asmlinkage long sys_ssmem_attach(int id,int flags,size_t length);
+asmlinkage long sys_ssmem_detach(void *addr);
 #endif
diff --git a/mm/Makefile b/mm/Makefile
index 89244cb..9508aef 100644
--- a/mm/Makefile
+++ b/mm/Makefile
@@ -11,7 +11,7 @@ ifdef CONFIG_CROSS_MEMORY_ATTACH
 mmu-$(CONFIG_MMU)	+= process_vm_access.o
 endif
 
-obj-y			:= filemap.o mempool.o oom_kill.o fadvise.o \
+obj-y			:= filemap.o mempool.o oom_kill.o fadvise.o ssmem.o \
 			   maccess.o page_alloc.o page-writeback.o \
 			   readahead.o swap.o truncate.o vmscan.o shmem.o \
 			   util.o mmzone.o vmstat.o backing-dev.o \
diff --git a/mm/memory.c b/mm/memory.c
index 61a262b..940e521 100644
--- a/mm/memory.c
+++ b/mm/memory.c
@@ -60,6 +60,8 @@
 #include <linux/migrate.h>
 #include <linux/string.h>
 
+#include <linux/ssmem.h>
+
 #include <asm/io.h>
 #include <asm/pgalloc.h>
 #include <asm/uaccess.h>
@@ -3220,7 +3222,7 @@ static int do_anonymous_page(struct mm_struct *mm, struct vm_area_struct *vma,
 		return VM_FAULT_SIGBUS;
 
 	/* Use the zero-page for reads */
-	if (!(flags & FAULT_FLAG_WRITE)) {
+	if ((!(flags & FAULT_FLAG_WRITE))&&(!(vma->ssmem_id))) {
 		entry = pte_mkspecial(pfn_pte(my_zero_pfn(address),
 						vma->vm_page_prot));
 		page_table = pte_offset_map_lock(mm, pmd, address, &ptl);
@@ -3232,7 +3234,23 @@ static int do_anonymous_page(struct mm_struct *mm, struct vm_area_struct *vma,
 	/* Allocate our own private page. */
 	if (unlikely(anon_vma_prepare(vma)))
 		goto oom;
+
+//my ssmem
+/*	if(vma->ssmem_id){
+		if(ssmem[vma->ssmem_id]->page==NULL){
+			page = alloc_zeroed_user_highpage_movable(vma, address);
+			ssmem[vma->ssmem_id]->page=page;
+		}
+		else{
+			page=ssmem[vma->ssmem_id]->page;
+		}
+	}
+	else{
+		page = alloc_zeroed_user_highpage_movable(vma, address);
+	}*/
+
 	page = alloc_zeroed_user_highpage_movable(vma, address);
+
 	if (!page)
 		goto oom;
 	/*
@@ -3254,7 +3272,13 @@ static int do_anonymous_page(struct mm_struct *mm, struct vm_area_struct *vma,
 		goto release;
 
 	inc_mm_counter_fast(mm, MM_ANONPAGES);
-	page_add_new_anon_rmap(page, vma, address);
+
+//my ssmem
+/*	if(vma->ssmem_id){
+		page_add_anon_rmap(page,vma,address);
+	}
+	else*/
+		page_add_new_anon_rmap(page, vma, address);
 setpte:
 	set_pte_at(mm, address, page_table, entry);
 
diff --git a/mm/mmap.c b/mm/mmap.c
index 25abb88..1dbfe3e 100644
--- a/mm/mmap.c
+++ b/mm/mmap.c
@@ -37,6 +37,8 @@
 #include <linux/notifier.h>
 #include <linux/memory.h>
 
+#include <linux/ssmem.h>
+
 #include <asm/uaccess.h>
 #include <asm/cacheflush.h>
 #include <asm/tlb.h>
@@ -1547,6 +1549,7 @@ munmap_back:
 	vma->vm_flags = vm_flags;
 	vma->vm_page_prot = vm_get_page_prot(vm_flags);
 	vma->vm_pgoff = pgoff;
+	vma->ssmem_id=0;
 	INIT_LIST_HEAD(&vma->anon_vma_chain);
 
 	error = -EINVAL;	/* when rejecting VM_GROWSDOWN|VM_GROWSUP */
@@ -2569,6 +2572,15 @@ int do_munmap(struct mm_struct *mm, unsigned long start, size_t len)
 	 * Remove the vma's, and unmap the actual pages
 	 */
 	detach_vmas_to_be_unmapped(mm, vma, prev, end);
+
+//my ssmem
+/*	if(vma->ssmem_id){
+		if(ssmem[vma->ssmem_id]->count==1){
+			unmap_region(mm, vma, prev, start, end);
+		}
+	else{
+		unmap_region(mm, vma, prev, start, end);
+	}*/
 	unmap_region(mm, vma, prev, start, end);
 
 	/* Fix up all other VM information */
